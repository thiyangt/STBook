# Introduction to Time Series Forecasting

```{r}
library(reticulate)
##py_config()
use_python("C:/Users/DELL/AppData/Local/Programs/Python/Python313/python.exe", required = TRUE)

```

## Notation

$\hat{Y}_{T+h|T}$ - The forecast of the time series 
$ùëå$ at time $T+h$, made using the information available up to time $T$.

## Simple time series forecasting techniques

1.  Average method

2.  Naive method/ random walk method

3.  Seasonal naive method

4.  Drift method

[Reading](https://otexts.com/fpp2/simple-methods.html)

## Example: Electricity Demand Forecasting

```{r}
#| echo: true
library(fable)
library(fpp2)
```

```{r, fig.width=10}
#| echo: true
aelec <- window(elec, start=1980)
autoplot(aelec)
```


```{r}
#| echo: true
# Plot some forecasts
autoplot(aelec) +
  autolayer(meanf(aelec, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(aelec, h=11),
    series="Na√Øve", PI=FALSE) +
  autolayer(snaive(aelec, h=11),
    series="Seasonal na√Øve", PI=FALSE) +
  ggtitle("Forecasts from Mean, NAIVE and SNAIVE") +
  xlab("Year") + ylab("Value") +
  guides(colour=guide_legend(title="Forecast"))
```

## Statistical Properties

### Mean function

Let ${X_1, X_2, ...}$ be a sequence of time index random variables.

The **mean function** of ${X_t}$ is

$$\mu_X(t)=E(X_t).$$

### Covariance function

The **covariance function** of ${X_t}$ is

$$\gamma_X(r, s)=Cov(X_r, X_s)=E[(X_r-\mu_X(r))(X_s-\mu_X(s))]$$

for all integers $(r)$ and $(s)$.


### Autocovariance function

The autocovariance function of ${X_t}$ at lag $(h)$ is defined by $$\gamma_X(h):=\gamma_X(h, 0)=\gamma(t+h, t)=Cov(X_{t+h}, X_t).$$

or

The autocovariance function of ${X_t}$ at lag $(h)$ is

$$\gamma_X(h)=Cov(X_{t+h}, X_t).$$ 

### Autocorrelation function

The autocorrelation function of ${X_t}$ at lag $(h)$ is

$$\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h}, X_t).$$

## Weekly stationary

A time series ${X_t}$ is called weekly stationary if

-   $\mu_X(t)$ is independent of $t$.

-   $\gamma_X(t+h, t)$ is independent of $(t)$ for each $(h)$.

In other words the statistical properties of the time series (mean, variance, autocorrelation, etc.) do not depend on the time at which the series is observed, that is no trend or seasonality. However, a time series with cyclic behaviour (but with no trend or seasonality) is stationary.

## Strict stationarity of a time series

A time series $\{X_t\}$ is called weekly stationary if the random vector $[X_1, X_2..., X_n]$ and $[X_{1+h}, X_{2+h}..., X_{n+h}]$ have the same joint distribution for all integers $(h)$ and $(n > 0)$.



## 1. independent and identically distributed (iid) noise

1.  no trend or seasonal component

2.  observations are independent and identically distributed (iid) random variables with zero mean.

3.  Notation: ${X_t} \sim IID(0, \sigma^2)$

4.  plays an important role as a building block for more complicated time series.

## 2. White noise

If ${X_t}$ is a sequence of uncorrelated random variables, each with zero mean and variance $\sigma^2$, then such a sequence is referred to as **white noise**.

## Every $(IID(0, \sigma^2)$ sequence is $(WN(0, \sigma^2)$ but not conversely. Why?

### 1. White Noise (WN)

A sequence $\{X_t\}$ is called **white noise** with mean $0$ and variance $\sigma^2$, written $WN(0, \sigma^2)$, if:

- $\mathbb{E}[X_t] = 0$ for all $t$.  
- $\mathrm{Var}(X_t) = \sigma^2$ for all $t$.  
- $\mathrm{Cov}(X_t, X_s) = 0$ for all $t \neq s$ (uncorrelated across time).  

Notice: *uncorrelated $\neq$ independent*.


### 2. i.i.d. $(0, \sigma^2)$

A sequence $\{X_t\}$ is $IID(0, \sigma^2)$ if:

- $\mathbb{E}[X_t] = 0$.  
- $\mathrm{Var}(X_t) = \sigma^2$.  
- $X_t$ are independent and identically distributed.  



### 3. Why every $IID(0, \sigma^2)$ is $WN(0, \sigma^2)$

- Independence $\;\Rightarrow\;$ zero correlation.  
- So, an i.i.d. sequence automatically satisfies the white noise conditions (same mean, same variance, no correlation).  

Therefore:

$$
IID(0, \sigma^2) \;\;\Rightarrow\;\; WN(0, \sigma^2).
$$

### 4. Why not conversely?

The reverse is not always true, because **white noise only requires uncorrelatedness, not full independence**.  

That means a sequence could be white noise but still have dependence in higher moments (nonlinear dependence).



## 5. Example of WN but not IID

Let $\{Z_t\}$ be i.i.d. $N(0,1)$. Define

$$
X_t = Z_t \cdot Z_{t-1}.
$$

Then:

- $\mathbb{E}[X_t] = 0$,  
- $\mathrm{Var}(X_t) = 1$,  
- For $t \neq s$, $\mathrm{Cov}(X_t, X_s) = 0$. ‚úÖ So it‚Äôs white noise.  

But the sequence is **not independent** (because $X_t$ depends on $Z_{t-1}$, which also appears in $X_{t-1}$).  

Thus,

$$
X_t \sim WN(0,1) \quad \text{but not} \quad IID(0,1).
$$


## Simulation example 

IID series

```{r}
#| echo: true
set.seed(123)

# Parameters
n <- 200        # length of series
sigma <- 1      # standard deviation

# IID(0, sigma^2) ~ Normal(0, sigma^2)
iid_seq <- rnorm(n, mean = 0, sd = sigma)

# Quick check
mean(iid_seq)      # should be ~0
var(iid_seq)       # should be ~sigma^2
acf(iid_seq)       # autocorrelations ~ 0

```

White noise

```{r}
#| echo: true
set.seed(123)

n <- 200
Z <- rnorm(n, mean = 0, sd = 1)

# Construct WN but not IID
wn_not_iid <- Z[-1] * Z[-n]   # X_t = Z_t * Z_{t-1}, length n-1

# Quick check
mean(wn_not_iid)        # ~0
var(wn_not_iid)         # ~1
acf(wn_not_iid)         # uncorrelated -> ACF ~ 0

```

Side-by-side visualisation

```{r}
par(mfrow = c(2,2))

plot.ts(iid_seq, main = "IID(0,1)", ylab = "X_t")
acf(iid_seq, main = "ACF: IID(0,1)")

plot.ts(wn_not_iid, main = "WN(0,1) but not IID", ylab = "X_t")
acf(wn_not_iid, main = "ACF: WN(0,1) but not IID")

```

## 3. Random walk

A random walk process is obtained by cumulatively summing iid random variables. If ${S_t, t=0, 1, 2, ...}$ is a random walk process, then $S_0 =0$

$S_1=0+X_1$

$S_2=0+X_1+X_2$

$...$

$S_t=X_1+X_2+...+X_t.$


**Question**

Is ${S_t, t=0, 1, 2, ...}$ a weak stationary process?

## Identifying non-stationarity in the mean

-   Using time series plot

-   ACF plot

    -   ACF of stationary time series will drop to relatively quickly.

    -   The ACF of non-stationary series decreases slowly.

    -   For non-stationary series, the ACF at lag 1 is often large and positive.



## Backshift notation:

$$BX_t=X_{t-1}$$

## Ordinary differencing

The first-order differencing can be defined as

$$\nabla X_t = X_t-X_{t-1}=X_t-BX_t=(1-B)X_t$$ where $\nabla=1-B$.

The second-order differencing

$$\nabla^2X_t=\nabla(\nabla X_t)=\nabla(X_t-X_{t-1})=\nabla X_t - \nabla X_{t-1}$$

$$\nabla X_t - \nabla X_{t-1}=(X_t-X_{t-1})-(X_{t-1}-X_{t-2})$$ 

In practice, we seldom need to go beyond second order differencing.

## Seasonal differencing

Differencing between an observation and the corresponding observation from the previous year.

$$\nabla_mX_t=X_t-X_{t-m}=(1-B^m)X_t$$ where $(m)$ is the number of seasons. For monthly, $(m=12)$, for quarterly $(m=4)$.

For monthly series

$$\nabla_{12}X_t=X_t-X_{t-12}$$

## Twice-differenced series

$$\nabla^2_{12}X_t=\nabla_{12}X_t-\nabla_{12}X_{t-1}$$ $$\nabla_{12}X_t-\nabla_{12}X_{t-1}=(X_t-X_{t-12})-(X_{t-1}-X_{t-13})$$ If seasonality is strong, the seasonal differencing should be done first.


## Deterministic trend vs Stochastic trend

### Deterministic trend

$$Y_t  = f(t) + \epsilon_t$$

where $\epsilon_t \sim iid(0, \sigma^2)$, $t = 1, 2, ...T$

Mean of the process is time dependent, but the variance of the process is constant.

A trend is deterministic if it is a nonrandom function of time.

### Stochastic trend

**1. Random walk**

$$Y_t = Y_{t-1} + \epsilon_t$$

-   Random walk has a stochastic trend.

-   Model behind naive method.

A trend is said to be stochastic if it is a random function of time.



**2. Random walk with drift**

$$Y_t = \alpha+  Y_{t-1} + \epsilon_t$$

-   Random walk with drift has a stochastic trend and a deterministic trend.

-   Model behind drift method.

## Random walk

$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= Y_0 + \epsilon_1 \\
         Y_2 &=  Y_1 + \epsilon_2=Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &=  Y_2 + \epsilon_3=Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &=Y_{t-1} + \epsilon_t=Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t = Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$

Mean: $E(Y_t) = Y_0$.

Variance: $Var(Y_t)=t \sigma^2$.

## Random walk with drift

$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= \alpha+Y_0 + \epsilon_1 \\
         Y_2 &= \alpha+ Y_1 + \epsilon_2=2 \alpha+Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &= \alpha+ Y_2 + \epsilon_3= 3 \alpha+ Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &= \alpha+Y_{t-1} + \epsilon_t= t \alpha+ Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t \\
          Y_t &= t \alpha + Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$


It has a *deterministic trend* $(Y_0 + t \alpha)$ and a *stochastic trend* $\sum_{i=1}^{t} \epsilon_t$.

Mean: $E(Y_t) = Y_0 + t\alpha$

Variance: $Var(Y_t) = t\sigma^2$.

There is a trend in both mean and variance.

## Common trend removal (de-trending) procedures

1.  Deterministic trend: Time-trend regression

    The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.

2.  Stochastic trend: Differencing

    The process is also known as a **Difference-stationary process**.
    
## Remove seasonality

Take seasonal differencing

## Example: Differencing on AirPassengers Data

The built-in AirPassengers dataset (monthly airline passengers, 1949‚Äì1960) has trend + seasonality.

```{r}
#| echo: true
# Load data
data("AirPassengers")
ts_data <- AirPassengers

#par(mfrow = c(3,2))

# 1. Original series
plot(ts_data, main = "Original Series", ylab = "Passengers")

acf(ts_data, main = "ACF: Original Series")

# 2. First difference (remove trend)
diff1 <- diff(ts_data, differences = 1)
plot(diff1, main = "1st Difference (Remove Trend)", ylab = "Difference")
acf(diff1, main = "ACF: 1st Difference")

# 3. Seasonal difference (lag = 12, remove seasonality)
diff_seasonal <- diff(diff1, lag = 12)
plot(diff_seasonal, main = "Seasonal Difference (Remove Seasonality)", ylab = "Difference")
acf(diff_seasonal, main = "ACF: Seasonal Difference")


# 3. Seasonal difference (lag = 12, remove seasonality from the original series)
diff_seasonal_only <- diff(ts_data, lag = 12)
plot(diff_seasonal_only, main = "Seasonal Difference (Remove Seasonality)", ylab = "Difference")
acf(diff_seasonal_only, main = "ACF: Seasonal Difference")

```


```{r}
#| echo: true
head(ts_data, 14)
head(diff1, 14)
head(diff_seasonal, 14)
head(diff_seasonal_only, 14)
```

## Notation: I(d)

Integrated to order $d$: Series can be made stationary by differencing $d$ times.

-   Known as $I(d)$ process.

**Question:** Show that random walk process is an $I(1)$ process.

The random walk process is called a unit root process. (If one of the roots turns out to be one, then the process is called unit root process.)

## Variance stabilization

Transform the series.

Eg:

-   Square root: $W_t = \sqrt{Y_t}$

-   Logarithm: $W_t = log({Y_t})$

    -   This very useful.

    -   Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.
    
```{r}
#| echo: true
log_ts <- log(ts_data)
plot(log_ts, main = "Log-Transformed Series", ylab = "log(Passengers)", col = "steelblue")
```