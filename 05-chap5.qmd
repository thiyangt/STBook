# Models for Nonstationary Series {#sec-ch5}


## Types of Nonstationarity and Remedies

### 1. Deterministic trend

- A **deterministic trend** is a predictable, non-random component such as a straight line or quadratic curve.  
  Example:  
 
  $$X_t = \alpha + \beta t + \epsilon_t$$

  where $\epsilon_t$ is stationary white noise.  
  
- Solution:  
  - Fit and remove the trend (regression detrending), or  
  - Use first differencing (removes linear trend).  
- After detrending, the residuals should be stationary.

```{r, echo=TRUE}
library(ggplot2)
set.seed(123)

# Parameters
n <- 100
alpha <- 10
beta <- 0.5

# Simulate deterministic trend with noise
t <- 1:n
epsilon <- rnorm(n, mean = 0, sd = 1)
X <- alpha + beta * t + epsilon
data <- data.frame(t, X)

# Fit regression for detrending
fit <- lm(X ~ t, data = data)
data$resid <- residuals(fit)

# First differencing
data$diffX <- c(NA, diff(data$X))
head(data)

# --- Plot 1: Original series with fitted trend ---
p1 <- ggplot(data, aes(x = t, y = X)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  labs(title = "Original Series with Deterministic Trend",
       x = "Time", y = expression(X[t])) +
  theme_minimal(base_size = 14)

# --- Plot 2: Residuals after regression detrending ---
p2 <- ggplot(data, aes(x = t, y = resid)) +
  geom_line(color = "darkgreen", size = 1) +
  labs(title = "Residuals after Removing Trend (Detrended Series)",
       x = "Time", y = "Residuals") +
  theme_minimal(base_size = 14)

# --- Plot 3: First differenced series ---
p3 <- ggplot(data, aes(x = t, y = diffX)) +
  geom_line(color = "purple", size = 1) +
  labs(title = "First Differenced Series",
       x = "Time", y = expression(Delta*X[t])) +
  theme_minimal(base_size = 14)

# Print all plots (Quarto will stack them nicely)
p1
p2
p3

```

### 2. Stochastic trend

- A **stochastic trend** arises from accumulated random shocks.  
  Example: Random walk  

  $$X_t = X_{t-1} + \epsilon_t$$
  
  where shocks accumulate over time.  
- Unlike deterministic trends, stochastic trends are unpredictable and keep evolving.  
- Apply non-seasonal differencing to remove stochastic trend.
- Remedy: First differencing usually removes the unit root, yielding stationarity.  
- Testing: Augmented Dickey–Fuller (ADF), Phillips–Perron, or KPSS tests.

**Key difference:**  
- Deterministic trend = predictable pattern (e.g., straight line).  
- Stochastic trend = unpredictable, driven by random shocks.  

**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$

Miss one observation

**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$

Miss two observations

### 3. Seasonality
- Deterministic seasonal pattern → add seasonal dummies or Fourier terms.  
- Stochastic seasonal trend → apply **seasonal differencing**:  
 
- Seasonal ARIMA (SARIMA) models combine regular and seasonal differencing.

**Seasonal differencing:** $Y_t - Y_{t-m}$

- To get rid from prominent seasonal components. 

-   For monthly, $m=12$, for quarterly, $m=4$.

- For monthly, we will loose first 12 observations

-   Seasonally differenced series will have $T-m$ observations. Usually we do not consider differencing more than twice.


### 4. Nonconstant variance

- Some series show increasing variability over time.  The data show different variation at different levels of
the series.
- Remedies:  
  - Variance-stabilizing transformations (log, square root, Box–Cox).  
  - For conditional heteroscedasticity → fit GARCH-type models.

**What differences do you notice?**


```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
library(forecast)
library(fpp2)
p1 <- autoplot(AirPassengers)
```

```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
p2 <- autoplot(cangas)
library(patchwork)
p1|p2
```

**Log transformation to stabilize variance**

```{r, echo=TRUE}
## Original dataset
p1 <- AirPassengers |>
  autoplot() +
  ggtitle("Original scale")

## Transformed dataset
AirPassengersBoxCox = BoxCox(AirPassengers, lambda = "auto")

p2 <- AirPassengersBoxCox |>
  autoplot() +
  ggtitle(paste0("Box Cox with lambda = ", 
        round(attributes(AirPassengersBoxCox)$lambda,2))
          )
p1|p2              
```


## Choosing the Order of Differencing

- Avoid **over-differencing** (adds unnecessary MA terms and inflates variance).  
- Visual inspection:  
  - Random walk–like series → first differencing.  
  - Strong seasonal cycle → seasonal differencing.  
- Statistical tests (ADF, KPSS) can guide the choice.  



## Models after Differencing
- Once stationarity is achieved, ARMA models can be fitted.  
- General class:  
  - $ARIMA(p,d,q)$  
  - Seasonal $ARIMA (p,d,q)(P,D,Q))_m$ 
- Sometimes include drift to capture mean shifts.  


## Seasonal components

-   The seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF and ACF.


**ARIMA(0,0,0)(0,0,1)12 will show**

-   a spike at lag 12 in the ACF but no other significant spikes.

-   The PACF will show exponential decay in the seasonal lags 12, 24, 36, . . . .

**ARIMA(0,0,0)(1,0,0)12 will show**

-   exponential decay in the seasonal lags of the ACF.

-   a single significant spike at lag 12 in the PACF.

### Practical Examples
- Simulated series with linear trend → stationary after first differencing.  

- Monthly series with annual seasonality → stationary after seasonal differencing (s=12).  

- Log-transformed economic series → removes variance growth, then differencing achieves stationarity.  


## Modelling steps

1.  Plot the data.

2.  Split time series into training, validation (optional), test.

3.  If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.

4.  If the data are non-stationary, take first differences of the data until the data are stationary.

5.  Examine the ACF/PACF to identify a suitable model.

6.  Try your chosen model(s), and to search for a better model.

7.  Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

8.  Once the residuals look like white noise, calculate forecasts.

## Example: Model Fitting

**Step 1: Plot data**



1.  Detect unusual observations in the data

2.  Detect non-stationarity by visual inspections of plots

Stationary series:

-   has a constant mean value and fluctuates around the mean.

-   constant variance.

-   no pattern predictable in the long-term.

**Step 2: Split time series into training and test sets**

```{r, echo=TRUE}
training.ap <- window(AirPassengers, end=c(1957, 12))
training.ap
test.ap <- window(AirPassengers, start=c(1958, 1))
test.ap
```


```{r, echo=TRUE}
autoplot(AirPassengers) + 
  geom_vline(xintercept = 1958, colour="forestgreen")

```

Training part

```{r, echo=TRUE}
autoplot(training.ap)
```

**Step 3: Apply transformations**

```{r, echo=TRUE}
log.airpassenger <- log(training.ap)
#log.airpassenger <- BoxCox(training.ap, lambda = 0)
autoplot(log.airpassenger)

```

**Step 4: Take difference series**


**Without differencing**


```{r, echo=TRUE}

autoplot(log.airpassenger)

```



```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}

ggAcf(log.airpassenger)

```


**With differencing**

Seasonal

```{r, echo=TRUE}

log.airpassenger |> diff(lag=12)  |> autoplot()

```

```{r, echo=TRUE}

log.airpassenger |> diff(lag=12)  |> ggAcf()

```

Non-seasonal

```{r, echo=TRUE}

log.airpassenger |> diff(lag=1)  |> autoplot()

```


```{r, echo=TRUE}

log.airpassenger |> diff(lag=1)  |> ggAcf()

```


Seasonal differencing + Non-seasonal differencing

```{r, echo=TRUE}

log.airpassenger |> diff(lag=12)  |> diff(lag=1) |> autoplot()

```


```{r, echo=TRUE}

log.airpassenger |> diff(lag=12)  |> 
  diff(lag=1) |> ggAcf()

```


**Testing for nonstationarity for the presence of unit roots**

-   Dickey and Fuller (DF) test

-   Augmented DF test

-   Phillips and Perron (PP) nonparametric test

-   Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test

KPSS test

**H0:** Series is level or trend stationary.

**H1:** Series is not stationary.

```{r, echo=TRUE}
library(urca)
diff.sdiff.log.passenger <- log.airpassenger |>
  diff(lag=12) |>
  diff(lag=1)

diff.sdiff.log.passenger |>
  ur.kpss() |>
  summary()
```

KPSS test

```{r, echo=TRUE}
ur.kpss(log.airpassenger) |> summary()
```

```{r, echo=TRUE}
sdiff.log.airpassenger <- training.ap |> log() |> diff(lag=12)
ur.kpss(sdiff.log.airpassenger) |> summary()
```





**Step 5: Examine the ACF/PACF to identify a suitable model**

-   $d=1$ and $D=1$ (from step 3)

-   Significant spike at lag 1 in ACF suggests non-seasonal MA(1) component.

-   Significant spike at lag 12 in ACF suggests seasonal MA(1) component.

-   Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.

-   By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.

-   Let's try both



**Initial model:**

$ARIMA(0,1,1)(0,1,1)_{12}$

$ARIMA(1,1,0)(1,1,0)_{12}$

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$

$ARIMA(1,1,1)(1,1,0)_{12}$

$ARIMA(1,1,1)(1,1,1)_{12}$

Both the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

$ARIMA(3,1,1)(1,1,1)_{12}$

$ARIMA(1,1,3)(1,1,1)_{12}$

$ARIMA(3,1,3)(1,1,1)_{12}$

AICc

**Initial model: AICc**

$ARIMA(0,1,1)(0,1,1)_{12}$: -344.33 (the smallest AICc)

$ARIMA(1,1,0)(1,1,0)_{12}$: -336.32

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$: -342.3 (second smallest AICc)

$ARIMA(1,1,1)(1,1,0)_{12}$: -336.08

$ARIMA(1,1,1)(1,1,1)_{12}$: -340.74

$ARIMA(3,1,1)(1,1,1)_{12}$: -338.89

$ARIMA(1,1,3)(1,1,1)_{12}$: -339.42

$ARIMA(3,1,3)(1,1,1)_{12}$: -335.65

**Step 7: Residual diagnostics**

Fitted values:

$\hat{Y}_{t|t-1}$: Forecast of $Y_t$ based on observations $Y_1,...Y_t$.

Residuals

$$e_t=Y_t - \hat{Y}_{t|t-1}$$

Assumptions of residuals

-   $\{e_t\}$ uncorrelated. If they aren't, then information left in residuals that should be used in computing forecasts.

<!--If you see autocorrelations, then you should go back and adjust residuals. In theoretically, If there is information leftover and we can do something better. But it is not the case you will also be able to do with. If can't you can't. Then stop. If you check you know you have done the best as you can.-->

-   $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

<!--If you see autocorrelations, then you should go back and adjust residuals. We want our residuals to be unbiased. If the mean is not zero. Go and adjust the model. Add an intercept. Whatever you want to do.-->

**Useful properties (for prediction intervals)**

-   $\{e_t\}$ have constant variance.

-   $\{e_t\}$ are normally distributed.

<!--If the following assumptions are wrong that doesn't mean your forecasts are incorrect. -->

**Step 7: Residual diagnostics (cont.)**

H0: Data are not serially correlated.

H1: Data are serially correlated.

```{r, comment=NA, warning=FALSE, message=FALSE,fig.height=3, echo=TRUE}
fit1 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(0,1,1), lambda = 0)
checkresiduals(fit1)
```


```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit1 %>% residuals() %>% ggtsdisplay()
```


```{r, comment=NA, warning=FALSE, message=FALSE, fig.height=4, echo=TRUE}
fit3 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(1,1,1), lambda = 0)
checkresiduals(fit3)
```


```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit3 %>% residuals() %>% ggtsdisplay()
```

**Step 8: Calculate forecasts**

$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit1 %>% forecast(h=36) %>% 
  autoplot()

```


$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit3 %>% forecast(h=36) %>% 
  autoplot()

```



$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit1.forecast <- fit1 %>% forecast(h=36) 
fit1.forecast$mean

```

$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}
fit3.forecast <- fit3 %>% forecast(h=36) 
fit3.forecast$mean
```

**Step 9: Evaluate forecast accuracy**

How well our model is doing for out-of-sample?

<!--So far we have talked about fitted values and residuals.-->

<!--Train data and Test data. We want to know if forecasts doing well for out-of-sample.-->

Forecast error = True value - Observed value

$$e_{T+h}=Y_{T+h}-\hat{Y}_{T+h|T}$$

Where,

$Y_{T+h}$: $(T+h)^{th}$ observation, $h=1,..., H$

$\hat{Y}_{T+h|T}$: Forecast based on data uo to time $T$.

-   **True** forecast error as the test data is not used in computing $\hat{Y}_{T+h|T}$.

-   Unlike, residuals, forecast errors on the test set involve multi-step forecasts.

-   Use forecast error measures to evaluate the models.

<!--Since, true forecast error, no hat involved.-->



$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}

fit1.forecast <- fit1 |> 
  forecast(h=36) 

```

```{r, comment=NA, echo=TRUE}
accuracy(fit1.forecast$mean, test.ap)
```

$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE, echo=TRUE}

fit3.forecast <- fit3 |>
  forecast(h=36) 

```

```{r, comment=NA, echo=TRUE}
accuracy(fit3.forecast$mean, test.ap)
```

$ARIMA(0,1,1)(0,1,1)_{12}$ MAE, MAPE is smaller than $ARIMA(0,1,1)(1,1,1)_{12}$. Hence, we select $ARIMA(0,1,1)(0,1,1)_{12}$ to forecast future values.

## `auto.arima`

Your turn: Explain the steps and logic behind the auto.arima() function in R for automatically identifying the appropriate ARIMA/SARIMA model.

## Reading

[https://jtr13.github.io/cc19/visualization-in-time-series-analysis.html](https://jtr13.github.io/cc19/visualization-in-time-series-analysis.html)