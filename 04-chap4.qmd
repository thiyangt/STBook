# Models For Stationary Time Series


In this chapter we will discuss family of autoregressive moving average (ARMA) time series models.

## General Linear Process

A \textbf{linear process} is a time series written as an infinite linear combination of random shocks $\{\varepsilon_t\}$:

$$X_t = \mu + \sum_{j=0}^{\infty} \psi_j \, \varepsilon_{t-j},$$

where,

  $\mu$ is the mean
  
  $\{\psi_j\}$ are coefficients (weights)
  
  $\varepsilon_t \sim \text{i.i.d. } (0, \sigma^2)$ are white noise shocks.


## Moving Average Processes

If only finitely many coefficients $\psi_j$ are nonzero, say up to lag $q$, then we have an MA($q$) process:


$$X_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},$$

where:

- $\mu$ is the mean,

- $\varepsilon_t \sim \text{i.i.d. }(0, \sigma^2)$ are white noise shocks,

- $\theta_1, \dots, \theta_q$ are the MA coefficients.

So we can write:


$$\text{MA}(q) \subset \text{Linear Process}.$$


## Autoregressive Processes

$Y_t = \alpha + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t$

Where:

$Y_t$ is the value at time $t$

$\alpha$ is a constant,

$\phi_1, \phi_2,...\phi_p$ are the parameters,

$\epsilon_t$ is white noise (error term),

$p$ is the order of the AR model.

## AR processes are also just special cases of the general linear process

If the AR process is causal (i.e., roots of the characteristic polynomial lie outside the unit circle), then it can be written as an infinite linear process:

$$X_t = \sum_{j=0}^{\infty} \psi_j \, \varepsilon_{t-j},$$

where:

- $\varepsilon_t \sim \text{i.i.d. }(0, \sigma^2)$ are white noise shocks,

- $\psi_j$ are coefficients determined from the AR parameters.

### AR(p) Process as an Infinite Linear Process Using Backshift Operator

Start with the AR(p) process:

$$X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \dots + \phi_p X_{t-p} + \varepsilon_t,$$

where $\varepsilon_t \sim \text{i.i.d. }(0, \sigma^2)$ are white noise shocks.


#### 1. Define the backshift operator \(B\)


$$B X_t = X_{t-1}, \quad B^2 X_t = X_{t-2}, \dots, B^p X_t = X_{t-p}.$$

#### 2. Rewrite the AR(p) process using \(B\)

$$X_t - \phi_1 B X_t - \phi_2 B^2 X_t - \dots - \phi_p B^p X_t = \varepsilon_t$$

Factor out \(X_t\):

$$(1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p) X_t = \varepsilon_t$$

Define the AR polynomial:

$$\phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p$$

Then the AR(p) process becomes:

$$\phi(B) X_t = \varepsilon_t$$



#### 3. Express as an infinite linear process

If the AR process is **causal** (roots of $\phi(z)=0$ lie outside the unit circle), we can invert the operator:

$$X_t = \phi(B)^{-1} \varepsilon_t$$

Expanding gives:

$$X_t = \sum_{j=0}^{\infty} \psi_j \, \varepsilon_{t-j},$$

where the coefficients $\psi_j$ are determined recursively from the AR parameters $\phi_1, \dots, \phi_p$.

This shows that causal AR processes are **special cases of the general linear process**.


## In-class: Properties of AR(1) process

Derive

-   Mean

-   Variance

-   Covariance

-   Autocorrelation function of an AR(1) process

## In-class: Properties of AR(2) process

Derive

-   Mean

-   Variance

-   Covariance

-   Autocorrelation function of an AR(1) process


## In-class: Properties of AR(P) process

Derive

-   Mean

-   Variance

-   Covariance

-   Autocorrelation function of an AR(P) process



