[
  {
    "objectID": "03-chap3.html",
    "href": "03-chap3.html",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "",
    "text": "3.1 Notation\n\\hat{Y}_{T+h|T} - The forecast of the time series ùëå at time T+h, made using the information available up to time T.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#simple-time-series-forecasting-techniques",
    "href": "03-chap3.html#simple-time-series-forecasting-techniques",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.2 Simple time series forecasting techniques",
    "text": "3.2 Simple time series forecasting techniques\n\nAverage method\nNaive method/ random walk method\nSeasonal naive method\nDrift method\n\nReading",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#example-electricity-demand-forecasting",
    "href": "03-chap3.html#example-electricity-demand-forecasting",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.3 Example: Electricity Demand Forecasting",
    "text": "3.3 Example: Electricity Demand Forecasting\n\nlibrary(fable)\nlibrary(fpp2)\n\n\naelec &lt;- window(elec, start=1980)\nautoplot(aelec)\n\n\n\n\n\n\n\n\n# Plot some forecasts\nautoplot(aelec) +\n  autolayer(meanf(aelec, h=11),\n    series=\"Mean\", PI=FALSE) +\n  autolayer(naive(aelec, h=11),\n    series=\"Na√Øve\", PI=FALSE) +\n  autolayer(snaive(aelec, h=11),\n    series=\"Seasonal na√Øve\", PI=FALSE) +\n  ggtitle(\"Forecasts from Mean, NAIVE and SNAIVE\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  guides(colour=guide_legend(title=\"Forecast\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#time-series-and-stochastic-processes",
    "href": "03-chap3.html#time-series-and-stochastic-processes",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.4 Time Series and Stochastic Processes",
    "text": "3.4 Time Series and Stochastic Processes\nThe terms stochastic processes and time series are closely related but not the same.\nA is a collection of random variables indexed by time (or space).\n\\{X_t : t \\in T\\},\nwhere T is the index set (e.g., discrete or continuous time).\nA is a single realization (observed data) of a stochastic process. It is the actual sequence of observations collected over time.\nIn short:\nStochastic process = model/theory (all possible sequences). The probability mechanism (all possible paths).\nTime series = observed data (one sequence). One observed path (the single trajectory we actually have).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#statistical-properties",
    "href": "03-chap3.html#statistical-properties",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.5 Statistical Properties",
    "text": "3.5 Statistical Properties\nMean function\nLet {X_1, X_2, ...} be a sequence of time index random variables.\nThe mean function of {X_t} is\n\\mu_X(t)=E(X_t).\nCovariance function\nThe covariance function of {X_t} is\n\\gamma_X(r, s)=Cov(X_r, X_s)=E[(X_r-\\mu_X(r))(X_s-\\mu_X(s))]\nfor all integers (r) and (s).\nAutocovariance function\nThe autocovariance function of {X_t} at lag (h) is defined by \\gamma_X(h):=\\gamma_X(h, 0)=\\gamma(t+h, t)=Cov(X_{t+h}, X_t).\nor\nThe autocovariance function of {X_t} at lag (h) is\n\\gamma_X(h)=Cov(X_{t+h}, X_t).\nAutocorrelation function\nThe autocorrelation function of {X_t} at lag (h) is\n\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}=Cor(X_{t+h}, X_t).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#weekly-stationary",
    "href": "03-chap3.html#weekly-stationary",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.6 Weekly stationary",
    "text": "3.6 Weekly stationary\nA time series {X_t} is called weekly stationary if\n\n\\mu_X(t) is independent of t.\nVar(X_t) = \\sigma^2, Variance is constant. 2\n\\gamma_X(t+h, t) is independent of (t) for each (h). The autocovariance depends only on the lag (\\gamma(h) depends only on how far apart two points are (h), and not on the actual time t.\n\nIn other words the statistical properties of the time series (mean, variance, autocorrelation, etc.) do not depend on the time at which the series is observed, that is no trend or seasonality. However, a time series with cyclic behaviour (but with no trend or seasonality) is stationary.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#strict-stationarity-of-a-time-series",
    "href": "03-chap3.html#strict-stationarity-of-a-time-series",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.7 Strict stationarity of a time series",
    "text": "3.7 Strict stationarity of a time series\nA time series \\{X_t\\} is called strictly stationary if the random vector [X_1, X_2..., X_n] and [X_{1+h}, X_{2+h}..., X_{n+h}] have the same joint distribution for all integers (h) and (n &gt; 0).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#independent-and-identically-distributed-iid-noise",
    "href": "03-chap3.html#independent-and-identically-distributed-iid-noise",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.8 1. independent and identically distributed (iid) noise",
    "text": "3.8 1. independent and identically distributed (iid) noise\n\nno trend or seasonal component\nobservations are independent and identically distributed (iid) random variables with zero mean.\nNotation: {X_t} \\sim IID(0, \\sigma^2)\nplays an important role as a building block for more complicated time series.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#white-noise",
    "href": "03-chap3.html#white-noise",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.9 2. White noise",
    "text": "3.9 2. White noise\nIf {X_t} is a sequence of uncorrelated random variables, each with zero mean and variance \\sigma^2, then such a sequence is referred to as white noise.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#every-iid0-sigma2-sequence-is-wn0-sigma2-but-not-conversely.-why",
    "href": "03-chap3.html#every-iid0-sigma2-sequence-is-wn0-sigma2-but-not-conversely.-why",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.10 Every (IID(0, \\sigma^2) sequence is (WN(0, \\sigma^2) but not conversely. Why?",
    "text": "3.10 Every (IID(0, \\sigma^2) sequence is (WN(0, \\sigma^2) but not conversely. Why?\n1. White Noise (WN)\nA sequence \\{X_t\\} is called white noise with mean 0 and variance \\sigma^2, written WN(0, \\sigma^2), if:\n\n\n\\mathbb{E}[X_t] = 0 for all t.\n\n\n\\mathrm{Var}(X_t) = \\sigma^2 for all t.\n\n\n\\mathrm{Cov}(X_t, X_s) = 0 for all t \\neq s (uncorrelated across time).\n\nNotice: uncorrelated \\neq independent.\n2. i.i.d. (0, \\sigma^2)\n\nA sequence \\{X_t\\} is IID(0, \\sigma^2) if:\n\n\n\\mathbb{E}[X_t] = 0.\n\n\n\\mathrm{Var}(X_t) = \\sigma^2.\n\n\nX_t are independent and identically distributed.\n3. Why every IID(0, \\sigma^2) is WN(0, \\sigma^2)\n\n\nIndependence \\;\\Rightarrow\\; zero correlation.\n\nSo, an i.i.d. sequence automatically satisfies the white noise conditions (same mean, same variance, no correlation).\n\nTherefore:\n\nIID(0, \\sigma^2) \\;\\;\\Rightarrow\\;\\; WN(0, \\sigma^2).\n\n4. Why not conversely?\nThe reverse is not always true, because white noise only requires uncorrelatedness, not full independence.\nThat means a sequence could be white noise but still have dependence in higher moments (nonlinear dependence).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#example-of-wn-but-not-iid",
    "href": "03-chap3.html#example-of-wn-but-not-iid",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.11 5. Example of WN but not IID",
    "text": "3.11 5. Example of WN but not IID\nLet \\{Z_t\\} be i.i.d. N(0,1). Define\n\nX_t = Z_t \\cdot Z_{t-1}.\n\nThen:\n\n\n\\mathbb{E}[X_t] = 0,\n\n\n\\mathrm{Var}(X_t) = 1,\n\nFor t \\neq s, \\mathrm{Cov}(X_t, X_s) = 0. ‚úÖ So it‚Äôs white noise.\n\nBut the sequence is not independent (because X_t depends on Z_{t-1}, which also appears in X_{t-1}).\nThus,\n\nX_t \\sim WN(0,1) \\quad \\text{but not} \\quad IID(0,1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#simulation-example",
    "href": "03-chap3.html#simulation-example",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.12 Simulation example",
    "text": "3.12 Simulation example\nIID series\n\nset.seed(123)\n\n# Parameters\nn &lt;- 200        # length of series\nsigma &lt;- 1      # standard deviation\n\n# IID(0, sigma^2) ~ Normal(0, sigma^2)\niid_seq &lt;- rnorm(n, mean = 0, sd = sigma)\n\n# Quick check\nmean(iid_seq)      # should be ~0\n\n[1] -0.008570445\n\nvar(iid_seq)       # should be ~sigma^2\n\n[1] 0.8895506\n\nacf(iid_seq)       # autocorrelations ~ 0\n\n\n\n\n\n\n\nWhite noise\n\nset.seed(123)\n\nn &lt;- 200\nZ &lt;- rnorm(n, mean = 0, sd = 1)\n\n# Construct WN but not IID\nwn_not_iid &lt;- Z[-1] * Z[-n]   # X_t = Z_t * Z_{t-1}, length n-1\n\n# Quick check\nmean(wn_not_iid)        # ~0\n\n[1] -0.05650406\n\nvar(wn_not_iid)         # ~1\n\n[1] 0.8196189\n\nacf(wn_not_iid)         # uncorrelated -&gt; ACF ~ 0\n\n\n\n\n\n\n\nSide-by-side visualisation",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#random-walk",
    "href": "03-chap3.html#random-walk",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.13 3. Random walk",
    "text": "3.13 3. Random walk\nA random walk process is obtained by cumulatively summing iid random variables. If {S_t, t=0, 1, 2, ...} is a random walk process, then S_0 =0\nS_1=0+X_1\nS_2=0+X_1+X_2\n...\nS_t=X_1+X_2+...+X_t.\nQuestion\nIs {S_t, t=0, 1, 2, ...} a weak stationary process?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#identifying-non-stationarity-in-the-mean",
    "href": "03-chap3.html#identifying-non-stationarity-in-the-mean",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.14 Identifying non-stationarity in the mean",
    "text": "3.14 Identifying non-stationarity in the mean\n\nUsing time series plot\n\nACF plot\n\nACF of stationary time series will drop to relatively quickly.\nThe ACF of non-stationary series decreases slowly.\nFor non-stationary series, the ACF at lag 1 is often large and positive.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#backshift-notation",
    "href": "03-chap3.html#backshift-notation",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.15 Backshift notation:",
    "text": "3.15 Backshift notation:\nBX_t=X_{t-1}",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#ordinary-differencing",
    "href": "03-chap3.html#ordinary-differencing",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.16 Ordinary differencing",
    "text": "3.16 Ordinary differencing\nThe first-order differencing can be defined as\n\\nabla X_t = X_t-X_{t-1}=X_t-BX_t=(1-B)X_t where \\nabla=1-B.\nThe second-order differencing\n\\nabla^2X_t=\\nabla(\\nabla X_t)=\\nabla(X_t-X_{t-1})=\\nabla X_t - \\nabla X_{t-1}\n\\nabla X_t - \\nabla X_{t-1}=(X_t-X_{t-1})-(X_{t-1}-X_{t-2})\nIn practice, we seldom need to go beyond second order differencing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#seasonal-differencing",
    "href": "03-chap3.html#seasonal-differencing",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.17 Seasonal differencing",
    "text": "3.17 Seasonal differencing\nDifferencing between an observation and the corresponding observation from the previous year.\n\\nabla_mX_t=X_t-X_{t-m}=(1-B^m)X_t where (m) is the number of seasons. For monthly, (m=12), for quarterly (m=4).\nFor monthly series\n\\nabla_{12}X_t=X_t-X_{t-12}",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#twice-differenced-series",
    "href": "03-chap3.html#twice-differenced-series",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.18 Twice-differenced series",
    "text": "3.18 Twice-differenced series\n\\nabla^2_{12}X_t=\\nabla_{12}X_t-\\nabla_{12}X_{t-1} \\nabla_{12}X_t-\\nabla_{12}X_{t-1}=(X_t-X_{t-12})-(X_{t-1}-X_{t-13}) If seasonality is strong, the seasonal differencing should be done first.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#deterministic-trend-vs-stochastic-trend",
    "href": "03-chap3.html#deterministic-trend-vs-stochastic-trend",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.19 Deterministic trend vs Stochastic trend",
    "text": "3.19 Deterministic trend vs Stochastic trend\nDeterministic trend\nY_t  = f(t) + \\epsilon_t\nwhere \\epsilon_t \\sim iid(0, \\sigma^2), t = 1, 2, ...T\nMean of the process is time dependent, but the variance of the process is constant.\nA trend is deterministic if it is a nonrandom function of time. A deterministic trend is a predictable, fixed function of time. If you know the form of the function, you can determine the trend exactly.\nStochastic trend\nA stochastic trend is driven by random shocks that accumulate over time. A stochastic trend is driven by random shocks (also called innovations, disturbances, or error terms) that accumulate over time.\n1. Random walk\nY_t = Y_{t-1} + \\epsilon_t\n\nRandom walk has a stochastic trend.\nModel behind naive method.\n\nA trend is said to be stochastic if it is a random function of time.\n2. Random walk with drift\nY_t = \\alpha +  Y_{t-1} + \\epsilon_t\n\nRandom walk with drift has a stochastic trend and a deterministic trend.\nModel behind drift method.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#random-walk-1",
    "href": "03-chap3.html#random-walk-1",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.20 Random walk",
    "text": "3.20 Random walk\n\n\\begin{aligned}\n  Y_t &= Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= Y_0 + \\epsilon_1 \\\\\n         Y_2 &=  Y_1 + \\epsilon_2=Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &=  Y_2 + \\epsilon_3=Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &=Y_{t-1} + \\epsilon_t=Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t = Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n\nMean: E(Y_t) = Y_0.\nVariance: Var(Y_t)=t \\sigma^2.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#random-walk-with-drift",
    "href": "03-chap3.html#random-walk-with-drift",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.21 Random walk with drift",
    "text": "3.21 Random walk with drift\n\n\\begin{aligned}\n  Y_t &= \\alpha + Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= \\alpha+Y_0 + \\epsilon_1 \\\\\n         Y_2 &= \\alpha+ Y_1 + \\epsilon_2=2 \\alpha+Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &= \\alpha+ Y_2 + \\epsilon_3= 3 \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &= \\alpha+Y_{t-1} + \\epsilon_t= t \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t \\\\\n          Y_t &= t \\alpha + Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n\nIt has a deterministic trend (Y_0 + t \\alpha) and a stochastic trend \\sum_{i=1}^{t} \\epsilon_t.\nMean: E(Y_t) = Y_0 + t\\alpha\nVariance: Var(Y_t) = t\\sigma^2.\nThere is a trend in both mean and variance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#common-trend-removal-de-trending-procedures",
    "href": "03-chap3.html#common-trend-removal-de-trending-procedures",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.22 Common trend removal (de-trending) procedures",
    "text": "3.22 Common trend removal (de-trending) procedures\n\n\nDeterministic trend: Time-trend regression\nThe trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.\n\n\nStochastic trend: Differencing\nThe process is also known as a Difference-stationary process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#remove-seasonality",
    "href": "03-chap3.html#remove-seasonality",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.23 Remove seasonality",
    "text": "3.23 Remove seasonality\nTake seasonal differencing",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#example-differencing-on-airpassengers-data",
    "href": "03-chap3.html#example-differencing-on-airpassengers-data",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.24 Example: Differencing on AirPassengers Data",
    "text": "3.24 Example: Differencing on AirPassengers Data\nThe built-in AirPassengers dataset (monthly airline passengers, 1949‚Äì1960) has trend + seasonality.\n\n# Load data\ndata(\"AirPassengers\")\nts_data &lt;- AirPassengers\n\n#par(mfrow = c(3,2))\n\n# 1. Original series\nplot(ts_data, main = \"Original Series\", ylab = \"Passengers\")\n\n\n\n\n\n\nacf(ts_data, main = \"ACF: Original Series\")\n\n\n\n\n\n\n# 2. First difference (remove trend)\ndiff1 &lt;- diff(ts_data, differences = 1)\nplot(diff1, main = \"1st Difference (Remove Trend)\", ylab = \"Difference\")\n\n\n\n\n\n\nacf(diff1, main = \"ACF: 1st Difference\")\n\n\n\n\n\n\n# 3. Seasonal difference (lag = 12, remove seasonality)\ndiff_seasonal &lt;- diff(diff1, lag = 12)\nplot(diff_seasonal, main = \"Seasonal Difference (Remove Seasonality)\", ylab = \"Difference\")\n\n\n\n\n\n\nacf(diff_seasonal, main = \"ACF: Seasonal Difference\")\n\n\n\n\n\n\n# 3. Seasonal difference (lag = 12, remove seasonality from the original series)\ndiff_seasonal_only &lt;- diff(ts_data, lag = 12)\nplot(diff_seasonal_only, main = \"Seasonal Difference (Remove Seasonality)\", ylab = \"Difference\")\n\n\n\n\n\n\nacf(diff_seasonal_only, main = \"ACF: Seasonal Difference\")\n\n\n\n\n\n\n\n\nhead(ts_data, 14)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126                                        \n\nhead(diff1, 14)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949       6  14  -3  -8  14  13   0 -12 -17 -15  14\n1950  -3  11  15                                    \n\nhead(diff_seasonal, 14)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1950       5   1  -3  -2  10   8   0   0  -8  -4  12\n1951   8  -6  13                                    \n\nhead(diff_seasonal_only, 14)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1950   3   8   9   6   4  14  22  22  22  14  10  22\n1951  30  24",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#notation-id",
    "href": "03-chap3.html#notation-id",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.25 Notation: I(d)",
    "text": "3.25 Notation: I(d)\nIntegrated to order d: Series can be made stationary by differencing d times.\n\nKnown as I(d) process.\n\nQuestion: Show that random walk process is an I(1) process.\nThe random walk process is called a unit root process. (If one of the roots turns out to be one, then the process is called unit root process.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "03-chap3.html#variance-stabilization",
    "href": "03-chap3.html#variance-stabilization",
    "title": "\n3¬† Introduction to Time Series Forecasting\n",
    "section": "\n3.26 Variance stabilization",
    "text": "3.26 Variance stabilization\nTransform the series.\nEg:\n\nSquare root: W_t = \\sqrt{Y_t}\n\nLogarithm: W_t = log({Y_t})\n\nThis very useful.\nInterpretable: Changes in a log value are relative (percent) changes on the original sclae.\n\n\n\n\nlog_ts &lt;- log(ts_data)\nplot(log_ts, main = \"Log-Transformed Series\", ylab = \"log(Passengers)\", col = \"steelblue\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Introduction to Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatio-Temporal Data Analysis",
    "section": "",
    "text": "Copyright notice\nProduced on 27 September 2025.\n¬© Thiyanga S. Talagala (2025).",
    "crumbs": [
      "Front matter"
    ]
  },
  {
    "objectID": "04-chap4.html",
    "href": "04-chap4.html",
    "title": "4¬† Models For Stationary Time Series",
    "section": "",
    "text": "4.1 General Linear Process\nIn this chapter we will discuss family of autoregressive moving average (ARMA) time series models.\nA is a time series written as an infinite linear combination of random shocks \\{\\varepsilon_t\\}:\nX_t = \\mu + \\sum_{j=0}^{\\infty} \\psi_j \\, \\varepsilon_{t-j},\nwhere,\n\\mu is the mean\n\\{\\psi_j\\} are coefficients (weights)\n\\varepsilon_t \\sim \\text{i.i.d. } (0, \\sigma^2) are white noise shocks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Models For Stationary Time Series</span>"
    ]
  },
  {
    "objectID": "04-chap4.html#autoregressive-models",
    "href": "04-chap4.html#autoregressive-models",
    "title": "4¬† Models For Stationary Time Series",
    "section": "4.2 Autoregressive Models",
    "text": "4.2 Autoregressive Models\nY_t = \\alpha + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t\nWhere:\nY_t is the value at time t\n\\alpha is a constant,\n\\phi_1, \\phi_2,...\\phi_p are the parameters,\n\\epsilon_t is white noise (error term),\np is the order of the AR model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Models For Stationary Time Series</span>"
    ]
  },
  {
    "objectID": "04-chap4.html#in-class-properties-of-ar1-process",
    "href": "04-chap4.html#in-class-properties-of-ar1-process",
    "title": "4¬† Models For Stationary Time Series",
    "section": "4.3 In-class: Properties of AR(1) process",
    "text": "4.3 In-class: Properties of AR(1) process\nDerive\n\nMean\nVariance\nCovariance\nAutocorrelation function of an AR(1) process",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Models For Stationary Time Series</span>"
    ]
  },
  {
    "objectID": "04-chap4.html#in-class-properties-of-ar2-process",
    "href": "04-chap4.html#in-class-properties-of-ar2-process",
    "title": "4¬† Models For Stationary Time Series",
    "section": "4.4 In-class: Properties of AR(2) process",
    "text": "4.4 In-class: Properties of AR(2) process\nDerive\n\nMean\nVariance\nCovariance\nAutocorrelation function of an AR(1) process",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Models For Stationary Time Series</span>"
    ]
  },
  {
    "objectID": "04-chap4.html#in-class-properties-of-arp-process",
    "href": "04-chap4.html#in-class-properties-of-arp-process",
    "title": "4¬† Models For Stationary Time Series",
    "section": "4.5 In-class: Properties of AR(P) process",
    "text": "4.5 In-class: Properties of AR(P) process\nDerive\n\nMean\nVariance\nCovariance\nAutocorrelation function of an AR(P) process",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Models For Stationary Time Series</span>"
    ]
  }
]